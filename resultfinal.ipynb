{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec363118-5ac6-4f44-a08e-2d94e839a3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VOTING CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "452ae4c1-7c4d-48b1-8771-22fc49c7a66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Prediction: OR\n",
      "Logistic Regression Prediction: CG\n",
      "SVM Prediction: CG\n",
      "Voting Classifier Prediction: CG\n",
      "Class Probabilities: [0.6533393 0.3466607]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import pickle\n",
    "\n",
    "# Load the saved Word2Vec model\n",
    "word2vec_model_path = \"C:/Users/ncssa/Downloads/word2vec_model.model\"\n",
    "word2vec_model = Word2Vec.load(word2vec_model_path)\n",
    "\n",
    "# Load the saved Voting Classifier model\n",
    "voting_model_path = \"C:/Users/ncssa/Downloads/voting_classifier_softtextmodel.pkl\"\n",
    "with open(voting_model_path, 'rb') as file:\n",
    "    voting_model = pickle.load(file)\n",
    "\n",
    "# Function to create a vector for a sentence using Word2Vec\n",
    "def sentence_to_vector(sentence, word2vec_model, vector_size):\n",
    "    words = sentence.split()\n",
    "    word_vectors = [word2vec_model.wv[word] for word in words if word in word2vec_model.wv]\n",
    "    if word_vectors:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "# Input text\n",
    "input_text = [\"It's a great product.\"]\n",
    "\n",
    "# Generate Word2Vec vectors for the input text\n",
    "vector_size = word2vec_model.vector_size  # Dimension of Word2Vec vectors\n",
    "input_vectors = np.array([sentence_to_vector(sentence, word2vec_model, vector_size) for sentence in input_text])\n",
    "\n",
    "# Step 1: Get predictions from individual models in the VotingClassifier\n",
    "rf_pred = voting_model.estimators_[0].predict(input_vectors)[0]\n",
    "lr_pred = voting_model.estimators_[1].predict(input_vectors)[0]\n",
    "svm_pred = voting_model.estimators_[2].predict(input_vectors)[0]\n",
    "\n",
    "# Step 2: Get the final prediction from the Voting Classifier\n",
    "final_pred = voting_model.predict(input_vectors)[0]  # Get the class prediction (numeric)\n",
    "predicted_probabilities = voting_model.predict_proba(input_vectors)[0]  # Get class probabilities\n",
    "\n",
    "# Ensure the predictions are numeric for consistent mapping\n",
    "if isinstance(final_pred, str):\n",
    "    final_pred = int(final_pred == \"OR\")\n",
    "\n",
    "# Map the predicted class to labels (numeric)\n",
    "label_mapping = {0: \"CG\", 1: \"OR\"}  # Map 0 to \"CG\" and 1 to \"OR\"\n",
    "final_pred_label = label_mapping[final_pred]\n",
    "\n",
    "# Print predictions from individual models and the final prediction\n",
    "print(f\"Random Forest Prediction: {label_mapping[rf_pred]}\")\n",
    "print(f\"Logistic Regression Prediction: {label_mapping[lr_pred]}\")\n",
    "print(f\"SVM Prediction: {label_mapping[svm_pred]}\")\n",
    "print(f\"Voting Classifier Prediction: {final_pred_label}\")\n",
    "print(f\"Class Probabilities: {predicted_probabilities}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08408397-5a8d-41ec-b58e-e75a7d273df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEEP LEARNING LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9892adc-d4a2-466c-9ab2-671b0ff5e1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ncssa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 'label_classes.npy' not found. Define classes manually.\n",
      "Tokens: ['it', \"'s\", 'a', 'great', 'product', '.']\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 461ms/step\n",
      "Predicted Class: CG\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "try:\n",
    "    nltk.download('punkt_tab')  # For newer NLTK versions\n",
    "except:\n",
    "    nltk.download('punkt')  # Fallback for classic tokenizer\n",
    "\n",
    "# Load the saved Word2Vec model\n",
    "word2vec_model = Word2Vec.load(\"C:/Users/ncssa/Downloads/word2vec_model.model\")\n",
    "\n",
    "# Load the saved LSTM model\n",
    "lstm_model = load_model('lstm_model.h5')\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "try:\n",
    "    # Attempt to load classes from the saved file\n",
    "    label_encoder.classes_ = np.load('label_classes.npy', allow_pickle=True)\n",
    "except FileNotFoundError:\n",
    "    # Fallback to manually define classes\n",
    "    print(\"Warning: 'label_classes.npy' not found. Define classes manually.\")\n",
    "    class_labels = ['OG', 'CG']  # Define classes as Original (OG) and Computer Generated (CG)\n",
    "    label_encoder.fit(class_labels)\n",
    "\n",
    "# Review to classify\n",
    "review = \"It's a great product.\"\n",
    "\n",
    "# Tokenize the review\n",
    "tokens = word_tokenize(review.lower())  # Convert to lowercase and tokenize\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Convert tokens to Word2Vec vectors\n",
    "vectors = [word2vec_model.wv[word] for word in tokens if word in word2vec_model.wv]\n",
    "\n",
    "# Aggregate word vectors into a sentence vector\n",
    "if vectors:\n",
    "    sentence_vector = np.mean(vectors, axis=0)\n",
    "else:\n",
    "    # Handle case where no tokens are in the Word2Vec vocabulary\n",
    "    sentence_vector = np.zeros(word2vec_model.vector_size)\n",
    "\n",
    "# Reshape the sentence vector for LSTM input\n",
    "sentence_vector_reshaped = sentence_vector.reshape(1, 1, -1)  # (1 sample, 1 timestep, vector_size)\n",
    "\n",
    "# Predict the class using the LSTM model\n",
    "prediction = lstm_model.predict(sentence_vector_reshaped)\n",
    "\n",
    "# Get the predicted class index\n",
    "predicted_class = np.argmax(prediction, axis=1)\n",
    "\n",
    "# Decode the predicted class to 'OG' or 'CG'\n",
    "decoded_class = label_encoder.inverse_transform(predicted_class)\n",
    "\n",
    "# Output the prediction\n",
    "print(\"Predicted Class:\", decoded_class[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db7a650a-111c-4612-8856-e3626483a3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining BERT and LSTM for hybrid model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8658240-6173-4e2a-b49b-f18f0919fc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 696ms/step\n",
      "Predicted sentiment: CG\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import numpy as np\n",
    "\n",
    "# Load the `.keras` file\n",
    "loaded_model = load_model(\"C:/Users/ncssa/Downloads/bert_lstm_model.keras\")\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# Load tokenizer and BERT model\n",
    "bert_model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = TFAutoModel.from_pretrained(bert_model_name)\n",
    "\n",
    "# Input text\n",
    "input_text = [\"It's a great product.\"]\n",
    "\n",
    "# Tokenize the input text\n",
    "max_length = 128\n",
    "input_tokens = tokenizer(input_text, padding=True, truncation=True, return_tensors=\"tf\", max_length=max_length)\n",
    "\n",
    "# Generate BERT embeddings\n",
    "input_embeddings = bert_model(input_tokens)\n",
    "input_pooled_output = input_embeddings['pooler_output']\n",
    "\n",
    "# Reshape for LSTM input\n",
    "input_embeddings_reshaped = input_pooled_output[:, np.newaxis, :]\n",
    "\n",
    "# Predict using the trained model\n",
    "predictions = loaded_model.predict(input_embeddings_reshaped)\n",
    "\n",
    "# Decode predictions\n",
    "predicted_class = np.argmax(predictions, axis=1)[0]\n",
    "label_mapping = {0: \"CG\", 1: \"OR\"}  # Adjust based on your dataset\n",
    "predicted_label = label_mapping[predicted_class]\n",
    "print(f\"Predicted sentiment: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d68171f-8272-4d12-ab80-5de64461cf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-trained models for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6448b81-3627-4f98-a56e-58063c8632f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully.\n",
      "Predicted label: CG\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "#Import Required Libraries\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "\n",
    "# Path to the saved model and tokenizer\n",
    "save_path = \"C:/Users/ncssa/Downloads/bert_trained_model\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(save_path)\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(save_path)\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully.\")\n",
    "\n",
    "# Sample text for prediction\n",
    "text = \"It's a great product\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = loaded_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Get model prediction\n",
    "with torch.no_grad():\n",
    "    outputs = loaded_model(**inputs)\n",
    "\n",
    "# Extract the predicted class (the class with the highest score)\n",
    "predicted_class = torch.argmax(outputs.logits, dim=1).item()\n",
    "\n",
    "# Map numeric output to 'OG' or 'CG'\n",
    "class_map = {0: 'OG', 1: 'CG'}\n",
    "\n",
    "# Print the predicted label\n",
    "predicted_label = class_map.get(predicted_class, \"Unknown\")\n",
    "print(f\"Predicted label: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e7364aa-80db-4cd9-873b-167b3cf677c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-trained models for sentiment classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e5e960d-8d5b-4d15-8aa3-831eef6c235f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully.\n",
      "Predicted label: positive\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Path to the saved model and tokenizer\n",
    "save_path = \"C:/Users/ncssa/Downloads/sentiment_analysis_finalmodel\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(save_path)\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(save_path)\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully.\")\n",
    "\n",
    "# Sample text for prediction\n",
    "text = \"It's a great product\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = loaded_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Get model prediction\n",
    "with torch.no_grad():\n",
    "    outputs = loaded_model(**inputs)\n",
    "\n",
    "# Extract the predicted class (the class with the highest score)\n",
    "predicted_class = torch.argmax(outputs.logits, dim=1).item()\n",
    "\n",
    "# Map numeric output to 'OG' or 'CG'\n",
    "class_map = {0: 'negative', 1: 'positive'}\n",
    "\n",
    "# Print the predicted label\n",
    "predicted_label = class_map.get(predicted_class, \"Unknown\")\n",
    "print(f\"Predicted label: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3af531-a230-411a-875d-5bc5d28a9e2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
